<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>MCT Quickstart Guideline for Pytorch models &mdash; MCT Documentation: ver 1.8.0</title>
      <link rel="stylesheet" href="../static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../static/css/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../static/documentation_options.js"></script>
        <script src="../static/doctools.js"></script>
        <script src="../static/sphinx_highlight.js"></script>
    <script src="../static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="API Docs" href="../api/experimental_api_docs/index.html" />
    <link rel="prev" title="MCT Quickstart Guideline for Keras models" href="quickstart_keras.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            MCT
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="visualization.html">Visualization within TensorBoard</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization.html#cosine-similarity-comparison">Cosine Similarity Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization.html#mixed-precision-configuration-bit-width">Mixed-precision Configuration Bit-width</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quickstart_keras.html">Quick start tutorial for Keras Post Training Quantization</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quick start tutorial for Pytorch Post Training Quantization</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/experimental_api_docs/index.html">API Documentation</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MCT</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">MCT Quickstart Guideline for Pytorch models</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/guidelines/quickstart_pytorch.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="mct-quickstart-guideline-for-pytorch-models">
<span id="ug-quickstart-pytorch"></span><h1>MCT Quickstart Guideline for Pytorch models<a class="headerlink" href="#mct-quickstart-guideline-for-pytorch-models" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>Here is an example of a code that shows how to use MCT with Pytorch models.</p>
<p>Import MCT and mobilenet_v2 from torchvision.models:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="n">mobilenet_v2</span>
<span class="kn">import</span> <span class="nn">model_compression_toolkit</span> <span class="k">as</span> <span class="nn">mct</span>
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Data preprocessing imports and functions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>

<span class="k">def</span> <span class="nf">np_to_pil</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Initialize data loader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the batch size of the images at each calibration iteration.</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Set the path to the folder of images to load and use for the representative dataset.</span>
<span class="c1"># Notice that the folder have to contain at least one image.</span>
<span class="n">folder</span> <span class="o">=</span> <span class="s1">&#39;/path/to/images/folder&#39;</span>

<span class="c1"># Create a representative data generator, which returns a list of images.</span>
<span class="c1"># The images can be preprocessed using a list of preprocessing functions.</span>
<span class="kn">from</span> <span class="nn">model_compression_toolkit</span> <span class="kn">import</span> <span class="n">FolderImageLoader</span>

<span class="n">image_data_loader</span> <span class="o">=</span> <span class="n">FolderImageLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span>
                                      <span class="n">preprocessing</span><span class="o">=</span><span class="p">[</span><span class="n">np_to_pil</span><span class="p">,</span>
                                                     <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
                                                         <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
                                                         <span class="n">transforms</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
                                                         <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                                         <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                                                                              <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]),</span>
                                                     <span class="p">])</span>
                                                     <span class="p">],</span>
                                      <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

<span class="c1"># Create a Callable representative dataset for calibration purposes.</span>
<span class="c1"># The function should be called without any arguments, and should return a list numpy arrays (array for each</span>
<span class="c1"># model&#39;s input).</span>
<span class="c1"># For example: A model has two input tensors - one with input shape of [3 X 32 X 32] and the second with</span>
<span class="c1"># an input shape of [3 X 224 X 224]. We calibrate the model using batches of 50 images.</span>
<span class="c1"># Calling representative_data_gen() should return a list</span>
<span class="c1"># of two numpy.ndarray objects where the arrays&#39; shapes are [(50, 3, 32, 32), (50, 3, 224, 224)].</span>
<span class="n">n_iter</span> <span class="o">=</span> <span class="mi">20</span>
<span class="k">def</span> <span class="nf">representative_data_gen</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">[</span><span class="n">image_data_loader</span><span class="o">.</span><span class="n">sample</span><span class="p">()]</span>
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Get a TargetPlatformCapabilities:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get a TargetPlatformModel object that models the hardware for the quantized model inference.</span>
<span class="c1"># The model determines the quantization methods to use during the MCT optimization process.</span>
<span class="c1"># Here, for example, we use the default model that is attached to a Pytorch</span>
<span class="c1"># layers representation.</span>
<span class="n">target_platform_cap</span> <span class="o">=</span> <span class="n">mct</span><span class="o">.</span><span class="n">get_target_platform_capabilities</span><span class="p">(</span><span class="s1">&#39;pytorch&#39;</span><span class="p">,</span> <span class="s1">&#39;default&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Run Post Training Quantization:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a model and quantize it using the representative_data_gen as the calibration images.</span>
<span class="c1"># Set the number of calibration iterations to 20.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mobilenet_v2</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># set core configuration with z threshold=16 (an algorithm for outlier removal)</span>
<span class="n">core_config</span> <span class="o">=</span> <span class="n">mct</span><span class="o">.</span><span class="n">CoreConfig</span><span class="p">(</span><span class="n">quantization_config</span><span class="o">=</span><span class="n">QuantizationConfig</span><span class="p">(</span><span class="n">z_threshold</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="c1"># run post training quantization on the model to get the quantized model output</span>
<span class="n">quantized_model</span><span class="p">,</span> <span class="n">quantization_info</span> <span class="o">=</span> <span class="n">mct</span><span class="o">.</span><span class="n">pytorch_post_training_quantization_experimental</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
                                                                                         <span class="n">representative_data_gen</span><span class="p">,</span>
                                                                                         <span class="n">core_config</span><span class="o">=</span><span class="n">core_config</span><span class="p">,</span>
                                                                                         <span class="n">target_platform_capabilities</span><span class="o">=</span><span class="n">target_platform_cap</span><span class="p">)</span>
</pre></div>
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="quickstart_keras.html" class="btn btn-neutral float-left" title="MCT Quickstart Guideline for Keras models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../api/experimental_api_docs/index.html" class="btn btn-neutral float-right" title="API Docs" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Sony Semiconductor Israel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>